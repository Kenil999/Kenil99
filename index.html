<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Prompt Enhancer – Gemma 2B</title>
<meta name="viewport" content="width=device-width, initial-scale=1">

<script src="https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm/dist/web-llm.min.js"></script>

<style>
body { font-family: Arial; padding: 20px; background: #fafafa; }
.box { background: white; padding: 15px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 20px; }
textarea { width: 100%; height: 140px; border: none; resize: none; outline: none; font-size: 16px; }
button { width: 100%; padding: 15px; background: black; color: white; border: none; border-radius: 10px; cursor: pointer; font-size: 18px; }
.hidden { display: none; }
</style>
</head>

<body>

<h2>AI Prompt Enhancer (Gemma 2B)</h2>

<div class="box">
<textarea id="rawPrompt" placeholder="Type your raw prompt..."></textarea>
</div>

<button onclick="refinePrompt()">Generate Optimized Prompt</button>

<div id="finalBox" class="box hidden"></div>

<script>
const SYSTEM_PROMPT = `
You are an Expert Prompt Engineer. Your task is to transform a raw user prompt into a maximally optimized, structured, model-ready prompt.

PROCESS:
1. Generate EXACTLY 3 short internal refinement questions to better understand the user's intent.
   • Do NOT show the questions to the user.
   • Do NOT ask the user to answer.
   • These questions are part of your reasoning.

2. Immediately answer the 3 questions yourself using reasonable assumptions that do not alter the user's intent.

3. Produce the final refined output in the following format and nothing else:

STRUCTURED OPTIMIZED PROMPT
AI Role:
Core Task:
Constraints:
Reasoning Instructions:
Output Format:

KEY IMPROVEMENTS
• bullet point
• bullet point
• bullet point

RULES:
• Never output HTML tags.
• Never add anything beyond the required structure.
• Never fabricate unrelated facts.
• Keep everything concise and purposeful.
`;

let chat;

// Load Gemma 2B Instruct
(async () => {
    chat = await webllm.createChatSession("gemma-2b-it-q4f16_1-MLC");
})();

async function runModel(prompt) {
    const reply = await chat.generate(prompt);
    return reply;
}

async function refinePrompt() {
    const raw = document.getElementById("rawPrompt").value.trim();
    if (!raw) return;

    document.getElementById("finalBox").classList.remove("hidden");
    document.getElementById("finalBox").innerHTML = "Loading model or generating output...";

    const fullPrompt = `${SYSTEM_PROMPT}

RAW USER PROMPT:
${raw}

Begin the refinement process now.`;

    const result = await runModel(fullPrompt);

    document.getElementById("finalBox").innerHTML =
        `<strong>Optimized Prompt:</strong><br><br>${result.replace(/\n/g,'<br>')}`;
}
</script>

</body>
</html>
